{
	"name": "AutoML_NYC_taxi",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpoolMedium",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2
		},
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chaxu-test/providers/Microsoft.Synapse/workspaces/chaxueus2/bigDataPools/sparkpoolMedium",
				"name": "sparkpoolMedium",
				"type": "Spark",
				"endpoint": "https://chaxueus2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolMedium",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# -------------------------------------------------------------------------------------------------\n",
					"# TODO: Remove this code cell when the latest version of the AML SDK is deployed to the base image.\n",
					"# -------------------------------------------------------------------------------------------------\n",
					"\n",
					"def parse_target(target, add_managed_dataset_prefix=False):\n",
					"    from azureml.data.azure_storage_datastore import AbstractAzureStorageDatastore\n",
					"    from azureml.data.azure_data_lake_datastore import AbstractADLSDatastore\n",
					"    from azureml.data.datapath import DataPath\n",
					"    #from azureml.data.constants import MANAGED_DATASET\n",
					"    MANAGED_DATASET = 'managed-dataset'\n",
					"\n",
					"    datastore = None\n",
					"    relative_path = None\n",
					"\n",
					"    if isinstance(target, AbstractAzureStorageDatastore) or isinstance(target, AbstractADLSDatastore):\n",
					"        datastore = target\n",
					"        relative_path = MANAGED_DATASET if add_managed_dataset_prefix else '/'\n",
					"    elif isinstance(target, DataPath):\n",
					"        datastore = target._datastore\n",
					"        relative_path = (MANAGED_DATASET if add_managed_dataset_prefix else '/') \\\n",
					"            if target.path_on_datastore is None else target.path_on_datastore\n",
					"    elif isinstance(target, tuple) and len(target) == 2:\n",
					"        datastore = target[0]\n",
					"        relative_path = target[1]\n",
					"    if not isinstance(datastore, AbstractAzureStorageDatastore) and not isinstance(datastore, AbstractADLSDatastore):\n",
					"        raise ValueError(\"The target type is not supported, target: {}\".format(target))\n",
					"\n",
					"    return datastore, relative_path\n",
					"\n",
					"\n",
					"def _set_spark_config(datastore):\n",
					"    from pyspark.sql import SparkSession\n",
					"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
					"    spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"    if datastore.datastore_type == AZURE_BLOB:\n",
					"        account_name = datastore.account_name\n",
					"        account_key = datastore.account_key\n",
					"        endpoint = datastore.endpoint\n",
					"        spark.conf.set('fs.azure.account.key.{}.blob.{}'.format(account_name, endpoint), account_key)\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
					"        account_name = datastore.account_name\n",
					"        client_id = datastore.client_id\n",
					"        client_secret = datastore.client_secret\n",
					"        endpoint = datastore.endpoint\n",
					"        tenant_id = datastore.tenant_id\n",
					"        authority_url = datastore.authority_url\n",
					"        prefix = \"fs.azure.account\"\n",
					"        provider = \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
					"        storage_account = \"{}.dfs.{}\".format(account_name, endpoint)\n",
					"        spark.conf.set(\"{}.auth.type.{}\".format(prefix, storage_account), \"OAuth\")\n",
					"        spark.conf.set(\"{}.oauth.provider.type.{}\".format(prefix, storage_account), provider)\n",
					"        spark.conf.set(\"{}.oauth2.client.id.{}\".format(prefix, storage_account), client_id)\n",
					"        spark.conf.set(\"{}.oauth2.client.secret.{}\".format(prefix, storage_account), client_secret)\n",
					"        spark.conf.set(\"{}.oauth2.client.endpoint.{}\".format(prefix, storage_account),\n",
					"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
					"        client_id = datastore.client_id\n",
					"        client_secret = datastore.client_secret\n",
					"        tenant_id = datastore.tenant_id\n",
					"        authority_url = datastore.authority_url\n",
					"        prefix = \"fs.adl\"  # dfs.adls deprecated\n",
					"        spark.conf.set(\"{}.oauth2.access.token.provider.type\".format(prefix), \"ClientCredential\")\n",
					"        spark.conf.set(\"{}.oauth2.client.id\".format(prefix), client_id)\n",
					"        spark.conf.set(\"{}.oauth2.credential\".format(prefix), client_secret)\n",
					"        spark.conf.set(\"{}.oauth2.refresh.url\".format(prefix),\n",
					"                       \"{}/{}/oauth2/token\".format(authority_url, tenant_id))\n",
					"    else:\n",
					"        raise ValueError(\n",
					"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
					"\n",
					"\n",
					"def _get_output_uri(datastore, path):\n",
					"    from azureml.data.constants import AZURE_BLOB, AZURE_DATA_LAKE_GEN2, AZURE_DATA_LAKE\n",
					"\n",
					"    if datastore.datastore_type == AZURE_BLOB:\n",
					"        output_uri = 'wasbs://{}@{}.blob.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
					"                                                       datastore.endpoint, path)\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE_GEN2:\n",
					"        output_uri = 'abfss://{}@{}.dfs.{}/{}'.format(datastore.container_name, datastore.account_name,\n",
					"                                                      datastore.endpoint, path)\n",
					"    elif datastore.datastore_type == AZURE_DATA_LAKE:\n",
					"        output_uri = 'adl://{}.azuredatalakestore.net/{}'.format(datastore.store_name, path)\n",
					"    else:\n",
					"        raise ValueError(\n",
					"            \"The datastore type {} is not supported.\".format(datastore.datastore_type))\n",
					"\n",
					"    return output_uri\n",
					"\n",
					"\n",
					"def write_spark_dataframe(spark_dataframe, datastore, relative_path_with_guid, show_progress):\n",
					"    console = get_progress_logger(show_progress)\n",
					"    _set_spark_config(datastore)\n",
					"    output_uri = _get_output_uri(datastore, relative_path_with_guid)\n",
					"\n",
					"    console(\"Writing spark dataframe to {}\".format(relative_path_with_guid))\n",
					"    spark_dataframe.write.mode(\"overwrite\").option(\"header\", \"true\").format(\"parquet\").save(output_uri)\n",
					"\n",
					"\n",
					"def get_progress_logger(show_progress):\n",
					"    import sys\n",
					"    console = sys.stdout\n",
					"\n",
					"    def log(message):\n",
					"        if show_progress:\n",
					"            console.write(\"{}\\n\".format(message))\n",
					"\n",
					"    return log\n",
					"\n",
					"def _check_type(arg, arg_name, expected_type):\n",
					"    if not isinstance(arg, expected_type):\n",
					"        raise ValueError(\"Expected {} of type {} but received {}\".format(arg_name, expected_type, type(arg)))\n",
					"\n",
					"def register_spark_dataframe(dataframe, target, name, description=None, tags=None, show_progress=True):\n",
					"        \"\"\"Create a dataset from spark dataframe.\n",
					"\n",
					"        :param dataframe: Required, in memory dataframe to be uploaded.\n",
					"        :type dataframe: pyspark.sql.DataFrame\n",
					"        :param target: Required, the datastore path where the dataframe parquet data will be uploaded to.\n",
					"            A guid folder will be generated under the target path to avoid conflict.\n",
					"        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
					"            or tuple(azureml.core.datastore.Datastore, str) object\n",
					"        :param name: Required, the name of the registered dataset.\n",
					"        :type name: str\n",
					"        :param description: Optional. A text description of the dataset. Defaults to None.\n",
					"        :type description: str\n",
					"        :param tags: Optional. Dictionary of key value tags to give the dataset. Defaults to None.\n",
					"        :type tags: dict[str, str]\n",
					"        :param show_progress: Optional, indicates whether to show progress of the upload in the console.\n",
					"            Defaults to be True.\n",
					"        :type show_progress: bool\n",
					"        :return: The registered dataset.\n",
					"        :rtype: azureml.data.TabularDataset\n",
					"        \"\"\"\n",
					"        from azureml.data.datapath import DataPath\n",
					"        from pyspark.sql import DataFrame\n",
					"        from uuid import uuid4\n",
					"\n",
					"        console = get_progress_logger(show_progress)\n",
					"\n",
					"        console(\"Validating arguments.\")\n",
					"        _check_type(dataframe, \"dataframe\", DataFrame)\n",
					"        _check_type(name, \"name\", str)\n",
					"        datastore, relative_path = parse_target(target, True)\n",
					"        console(\"Arguments validated.\")\n",
					"\n",
					"        guid = uuid4()\n",
					"        relative_path_with_guid = \"{}/{}\".format(relative_path, guid)\n",
					"        write_spark_dataframe(dataframe, datastore, relative_path_with_guid, show_progress)\n",
					"\n",
					"        console(\"Creating new dataset\")\n",
					"        datapath = DataPath(datastore, \"/{}/*.parquet\".format(relative_path_with_guid))\n",
					"        saved_dataset = Dataset.Tabular.from_parquet_files(datapath)\n",
					"\n",
					"        console(\"Registering new dataset\")\n",
					"        registered_dataset = saved_dataset.register(datastore.workspace, name,\n",
					"                                                    description=description,\n",
					"                                                    tags=tags,\n",
					"                                                    create_new_version=True)\n",
					"        console(\"Successfully created and registered a new dataset.\")\n",
					"\n",
					"        return registered_dataset\n",
					""
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"import azureml.core\n",
					"\n",
					"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
					"from azureml.train.automl import AutoMLConfig"
				],
				"attachments": null,
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"subscription_id = \"051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3\"\n",
					"resource_group = \"chaxu-test\"\n",
					"workspace_name = \"chaxuamleus\"\n",
					"experiment_name = \"nyc_taxi_automl_run1\"\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"experiment = Experiment(ws, experiment_name)"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.sql(\"SELECT * FROM default.nyc_tlc\")\n",
					"\n",
					"datastore = Datastore.get_default(ws)\n",
					"dataset = register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")\n",
					"dataset_train, dataset_test = dataset.random_split(percentage = 0.8)"
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"automl_config = AutoMLConfig(spark_context = sc,\n",
					"                             task = \"classification\",\n",
					"                             training_data = dataset_train,\n",
					"                             label_column_name = \"paymentType\",\n",
					"                             primary_metric = \"accuracy\",\n",
					"                             experiment_timeout_hours = 3,\n",
					"                             max_concurrent_iterations = 2,\n",
					"                             enable_onnx_compatible_models = True)"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"run = experiment.submit(automl_config)"
				],
				"attachments": null,
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"run.wait_for_completion()"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"model = run.register_model(model_name = 'best_model', model_path = 'outputs/model.pkl')\r\n",
					""
				],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}